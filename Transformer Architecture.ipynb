{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f78c6074-97b3-47a6-becd-368ed74e078c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (25.0.1)\n",
      "Requirement already satisfied: setuptools in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (75.8.0)\n",
      "Requirement already satisfied: wheel in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (0.45.1)\n",
      "Requirement already satisfied: numpy in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (2.0.2)\n",
      "Requirement already satisfied: pandas in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (3.9.4)\n",
      "Requirement already satisfied: tensorflow>=2.10.1 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (2.19.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 4)) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 4)) (2025.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 5)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 5)) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 5)) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 5)) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 5)) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 5)) (3.2.1)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 5)) (6.5.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow>=2.10.1->-r requirements.txt (line 6)) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow>=2.10.1->-r requirements.txt (line 6)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow>=2.10.1->-r requirements.txt (line 6)) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow>=2.10.1->-r requirements.txt (line 6)) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow>=2.10.1->-r requirements.txt (line 6)) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow>=2.10.1->-r requirements.txt (line 6)) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow>=2.10.1->-r requirements.txt (line 6)) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow>=2.10.1->-r requirements.txt (line 6)) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow>=2.10.1->-r requirements.txt (line 6)) (2.32.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow>=2.10.1->-r requirements.txt (line 6)) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow>=2.10.1->-r requirements.txt (line 6)) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow>=2.10.1->-r requirements.txt (line 6)) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow>=2.10.1->-r requirements.txt (line 6)) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow>=2.10.1->-r requirements.txt (line 6)) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow>=2.10.1->-r requirements.txt (line 6)) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow>=2.10.1->-r requirements.txt (line 6)) (3.9.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow>=2.10.1->-r requirements.txt (line 6)) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow>=2.10.1->-r requirements.txt (line 6)) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow>=2.10.1->-r requirements.txt (line 6)) (0.37.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->-r requirements.txt (line 5)) (3.21.0)\n",
      "Requirement already satisfied: rich in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from keras>=3.5.0->tensorflow>=2.10.1->-r requirements.txt (line 6)) (13.9.4)\n",
      "Requirement already satisfied: namex in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from keras>=3.5.0->tensorflow>=2.10.1->-r requirements.txt (line 6)) (0.0.8)\n",
      "Requirement already satisfied: optree in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from keras>=3.5.0->tensorflow>=2.10.1->-r requirements.txt (line 6)) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow>=2.10.1->-r requirements.txt (line 6)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow>=2.10.1->-r requirements.txt (line 6)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow>=2.10.1->-r requirements.txt (line 6)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow>=2.10.1->-r requirements.txt (line 6)) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorboard~=2.19.0->tensorflow>=2.10.1->-r requirements.txt (line 6)) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorboard~=2.19.0->tensorflow>=2.10.1->-r requirements.txt (line 6)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorboard~=2.19.0->tensorflow>=2.10.1->-r requirements.txt (line 6)) (3.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard~=2.19.0->tensorflow>=2.10.1->-r requirements.txt (line 6)) (8.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow>=2.10.1->-r requirements.txt (line 6)) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from rich->keras>=3.5.0->tensorflow>=2.10.1->-r requirements.txt (line 6)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from rich->keras>=3.5.0->tensorflow>=2.10.1->-r requirements.txt (line 6)) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow>=2.10.1->-r requirements.txt (line 6)) (0.1.2)\n",
      "Requirement already satisfied: tensorflow in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow) (2.0.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorboard~=2.19.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard~=2.19.0->tensorflow) (8.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.19.0->tensorflow) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/sudarshansuresh/miniconda3/envs/tf-mac/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install -r requirements.txt\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1c9410e-f3a5-4993-a9c2-2c3204247b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from tensorflow.keras.layers import Layer, Embedding, Dense, LayerNormalization, Dropout\n",
    "import numpy as np\n",
    "\n",
    "#load the harry potter book as the dataset ->  url - https://www.kaggle.com/datasets/shubhammaindola/harry-potter-books\n",
    "def load_data(file_path):\n",
    "  with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "  return text\n",
    "\n",
    "file_path = \"hp_1.txt\"\n",
    "text = load_data(file_path).lower()\n",
    "\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts([text])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "\n",
    "# Convert text to sequences\n",
    "input_sequences = []\n",
    "tokens = tokenizer.texts_to_sequences([text])[0]\n",
    "seq_length = 50\n",
    "\n",
    "# First seq_length tokens (input): Used for training the model.\n",
    "# Last token (target): Used as the label the model tries to predict.\n",
    "# so total of (50 + 1) in one input_sequence index\n",
    "\n",
    "for i in range(seq_length, len(tokens)):\n",
    "    input_sequences.append(tokens[i - seq_length:i + 1])\n",
    "\n",
    "#print(input_sequences[0])\n",
    "\n",
    "# Pad sequences and split inputs/targets\n",
    "# after this X will have inputs and y will have label for those inputs\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=seq_length + 1, padding='pre'))\n",
    "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "\n",
    "# One-hot encode the labels , note- there are other ways for\n",
    "# encoding like pre-trained word2vec encoding and so on\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b77f84d8-ada5-4f73-913b-105d8990a25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Embedding, Dense, LayerNormalization, Dropout\n",
    "\n",
    "class MultiHeadAttention(Layer):\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads # example - 8\n",
    "\n",
    "        self.embed_dim = embed_dim # example - 512\n",
    "        # embed_dim = dimension of Q, K, and V before splitting into multiple heads\n",
    "        # It is same as total dimension of the input embeddings (word embeddings)\n",
    "\n",
    "        self.projection_dim = embed_dim // num_heads # Size of Each Attention Head's Subspace\n",
    "        # Each head gets a smaller subspace of the embedding dimension\n",
    "        # example - 64\n",
    "\n",
    "        # Fully connected (dense) layers that project the input into Q,K,V\n",
    "        # These layers map the input embeddings to the same embed_dim\n",
    "        # These layers will be reshaped / split later to split across attention heads\n",
    "        # A single large matrix multiplication is more efficient than many small ones\n",
    "        # GPUs love large matrix multiplications because they are optimized for parallel computation\n",
    "        # This allows TF/Keras to efficiently batch the computation, leveraging better GPU memory utilization\n",
    "\n",
    "        self.query_dense = Dense(embed_dim) # Q Determines \"what to focus on\"\n",
    "        self.key_dense = Dense(embed_dim) # K Acts as \"labels\" to be matched with queries\n",
    "        self.value_dense = Dense(embed_dim) # V Holds the actual information\n",
    "\n",
    "        self.combine_heads = Dense(embed_dim)\n",
    "        # After multi-head attention is applied, the outputs from all heads are concatenated back into embed_dim\n",
    "\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        scores = tf.matmul(query, key, transpose_b=True)\n",
    "        scores /= tf.math.sqrt(tf.cast(self.projection_dim, tf.float32)) # converting integer to a float32 tensor\n",
    "\n",
    "        attention_probs = tf.nn.softmax(scores, axis=-1) # how much attention each token should give to other tokens\n",
    "        # The higher the score, the more focus that token gets\n",
    "        # Softmax should be applied along the keys (i.e., across the last dimension of the scores matrix)\n",
    "        # Each row corresponds to a query token attending to all key tokens\n",
    "        # This ensures that each query distributes its attention across all keys properly\n",
    "        # Each row sums to 1\n",
    "\n",
    "        return tf.matmul(attention_probs, value), attention_probs\n",
    "\n",
    "    # x - query, key or value with shape - (batch_size, seq_len, embed_dim)\n",
    "    # batch_size - number of sequences being processed in parallel (for batch processing)\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        # before transpose - (batch_size, seq_len, num_heads, projection_dim)\n",
    "        # after transpose - (batch_size, num_heads, seq_len, projection_dim)\n",
    "        # The -1 in tf.reshape is a placeholder that tells TensorFlow to automatically\n",
    "        # infer that dimension's value based on the total number of elements in the tensor\n",
    "        # -1 is replaced by seq_len by tensorflow\n",
    "\n",
    "    # In TF,Keras - call(self, inputs) is a standard method used inside Layer subclasses\n",
    "    # to define the forward pass of a neural network layer\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value = inputs\n",
    "        batch_size = tf.shape(query)[0] # (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        query = self.split_heads(self.query_dense(query), batch_size)\n",
    "        key = self.split_heads(self.key_dense(key), batch_size)\n",
    "        value = self.split_heads(self.value_dense(value), batch_size)\n",
    "\n",
    "        attention, _ = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        # before transpose - (batch_size, num_heads, seq_len, projection_dim)\n",
    "        # after transpose -  (batch_size, seq_len, num_heads, projection_dim)\n",
    "\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "        # Merges all heads back into a single vector\n",
    "        # (batch_size, seq_len, num_heads, projection_dim) → (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        return self.combine_heads(concat_attention)\n",
    "\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),\n",
    "            Dense(embed_dim),\n",
    "        ])\n",
    "        # y = (x - mean) / root(variance + epsilon)\n",
    "        # epsilon ensures we never divide by zero\n",
    "        # it is small enough not to affect the result but large enough to prevent instability\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att([inputs, inputs, inputs])\n",
    "\n",
    "        # Dropout randomly deactivates some neurons during training to reduce overfitting\n",
    "        # Ensure dropout is only applied during training, not inference\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output) # Residual Connection\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output) # Residual Connection\n",
    "\n",
    "class TokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        # The Embedding layer takes an integer tensor and replaces each integer with an embed_dim-sized vector\n",
    "        # example - positions = [0, 1, 2, 3]\n",
    "        # after embedding - positions = [\n",
    "        #   [0.2, 0.1, 0.3, 0.5, 0.6, 0.9, 0.7, 0.8],  # Position 0\n",
    "        #   [0.4, 0.2, 0.1, 0.6, 0.5, 0.7, 0.9, 0.3],  # Position 1\n",
    "        #   [0.5, 0.3, 0.8, 0.2, 0.7, 0.4, 0.6, 0.1],  # Position 2\n",
    "        #   [0.9, 0.6, 0.2, 0.3, 0.1, 0.8, 0.4, 0.7]   # Position 3\n",
    "        # ]\n",
    "\n",
    "        # initial shape of x - (batch_size, seq_len)\n",
    "        # batch_size: Number of sentences in a batch\n",
    "        # seq_len: Number of tokens (words) in each sentence\n",
    "        # Each value in x is an integer index from 0 to vocab_size - 1\n",
    "        # after embedding - (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        # example - embed_dim = 8, batch_size = 2\n",
    "        #     x = [\n",
    "        #   [ [0.2, 0.1, 0.4, 0.3, 0.8, 0.7, 0.6, 0.9],  # Token 2\n",
    "        #     [0.5, 0.3, 0.9, 0.1, 0.2, 0.6, 0.8, 0.7],  # Token 5\n",
    "        #     [0.4, 0.9, 0.2, 0.3, 0.1, 0.7, 0.5, 0.6],  # Token 1\n",
    "        #     [0.3, 0.8, 0.6, 0.2, 0.5, 0.9, 0.7, 0.4]   # Token 7\n",
    "        #   ],  # First sentence\n",
    "\n",
    "        #   [ [0.1, 0.6, 0.9, 0.7, 0.3, 0.5, 0.2, 0.8],  # Token 0\n",
    "        #     [0.4, 0.2, 0.3, 0.9, 0.7, 0.5, 0.1, 0.6],  # Token 3\n",
    "        #     [0.8, 0.5, 0.4, 0.1, 0.6, 0.3, 0.2, 0.7],  # Token 8\n",
    "        #     [0.9, 0.3, 0.5, 0.7, 0.8, 0.2, 0.6, 0.1]   # Token 4\n",
    "        #   ]   # Second sentence\n",
    "        # ]\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        # the maximum sequence length the model can handle\n",
    "        maxlen = tf.shape(x)[-1] # sets maxlen to the length of the input sequence\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1) # Generate [0, 1, 2, ..., maxlen-1]\n",
    "        positions = self.pos_emb(positions) # Each position index is mapped to a trainable embedding of shape (maxlen, embed_dim)\n",
    "        x = self.token_emb(x) # Each token ID in x is mapped to an embedding of shape (batch_size, maxlen, embed_dim)\n",
    "        return x + positions\n",
    "\n",
    "        # x has shape (batch_size, seq_len, embed_dim)\n",
    "        # positions has shape (maxlen, embed_dim)\n",
    "        # But maxlen == seq_len, so positions effectively has shape (seq_len, embed_dim).\n",
    "        # TensorFlow broadcasts positions across batch_size, treating it as if it were (1, seq_len, embed_dim).\n",
    "        # This allows element-wise addition between x and position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c68e2729-38fe-47a8-a362-d86ccedeb1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, Embedding, Dense, LayerNormalization, Dropout\n",
    "\n",
    "class MultiHeadAttention(Layer):\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads # example - 8\n",
    "\n",
    "        self.embed_dim = embed_dim # example - 512\n",
    "        # embed_dim = dimension of Q, K, and V before splitting into multiple heads\n",
    "        # It is same as total dimension of the input embeddings (word embeddings)\n",
    "\n",
    "        self.projection_dim = embed_dim // num_heads # Size of Each Attention Head's Subspace\n",
    "        # Each head gets a smaller subspace of the embedding dimension\n",
    "        # example - 64\n",
    "\n",
    "        # Fully connected (dense) layers that project the input into Q,K,V\n",
    "        # These layers map the input embeddings to the same embed_dim\n",
    "        # These layers will be reshaped / split later to split across attention heads\n",
    "        # A single large matrix multiplication is more efficient than many small ones\n",
    "        # GPUs love large matrix multiplications because they are optimized for parallel computation\n",
    "        # This allows TF/Keras to efficiently batch the computation, leveraging better GPU memory utilization\n",
    "\n",
    "        self.query_dense = Dense(embed_dim) # Q Determines \"what to focus on\"\n",
    "        self.key_dense = Dense(embed_dim) # K Acts as \"labels\" to be matched with queries\n",
    "        self.value_dense = Dense(embed_dim) # V Holds the actual information\n",
    "\n",
    "        self.combine_heads = Dense(embed_dim)\n",
    "        # After multi-head attention is applied, the outputs from all heads are concatenated back into embed_dim\n",
    "\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        scores = tf.matmul(query, key, transpose_b=True)\n",
    "        scores /= tf.math.sqrt(tf.cast(self.projection_dim, tf.float32)) # converting integer to a float32 tensor\n",
    "\n",
    "        attention_probs = tf.nn.softmax(scores, axis=-1) # how much attention each token should give to other tokens\n",
    "        # The higher the score, the more focus that token gets\n",
    "        # Softmax should be applied along the keys (i.e., across the last dimension of the scores matrix)\n",
    "        # Each row corresponds to a query token attending to all key tokens\n",
    "        # This ensures that each query distributes its attention across all keys properly\n",
    "        # Each row sums to 1\n",
    "\n",
    "        return tf.matmul(attention_probs, value), attention_probs\n",
    "\n",
    "    # x - query, key or value with shape - (batch_size, seq_len, embed_dim)\n",
    "    # batch_size - number of sequences being processed in parallel (for batch processing)\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        # before transpose - (batch_size, seq_len, num_heads, projection_dim)\n",
    "        # after transpose - (batch_size, num_heads, seq_len, projection_dim)\n",
    "        # The -1 in tf.reshape is a placeholder that tells TensorFlow to automatically\n",
    "        # infer that dimension's value based on the total number of elements in the tensor\n",
    "        # -1 is replaced by seq_len by tensorflow\n",
    "\n",
    "    # In TF,Keras - call(self, inputs) is a standard method used inside Layer subclasses\n",
    "    # to define the forward pass of a neural network layer\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value = inputs\n",
    "        batch_size = tf.shape(query)[0] # (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        query = self.split_heads(self.query_dense(query), batch_size)\n",
    "        key = self.split_heads(self.key_dense(key), batch_size)\n",
    "        value = self.split_heads(self.value_dense(value), batch_size)\n",
    "\n",
    "        attention, _ = self.attention(query, key, value)\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        # before transpose - (batch_size, num_heads, seq_len, projection_dim)\n",
    "        # after transpose -  (batch_size, seq_len, num_heads, projection_dim)\n",
    "\n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim))\n",
    "        # Merges all heads back into a single vector\n",
    "        # (batch_size, seq_len, num_heads, projection_dim) → (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        return self.combine_heads(concat_attention)\n",
    "\n",
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"),\n",
    "            Dense(embed_dim),\n",
    "        ])\n",
    "        # y = (x - mean) / root(variance + epsilon)\n",
    "        # epsilon ensures we never divide by zero\n",
    "        # it is small enough not to affect the result but large enough to prevent instability\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att([inputs, inputs, inputs])\n",
    "\n",
    "        # Dropout randomly deactivates some neurons during training to reduce overfitting\n",
    "        # Ensure dropout is only applied during training, not inference\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output) # Residual Connection\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output) # Residual Connection\n",
    "\n",
    "class TokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        # The Embedding layer takes an integer tensor and replaces each integer with an embed_dim-sized vector\n",
    "        # example - positions = [0, 1, 2, 3]\n",
    "        # after embedding - positions = [\n",
    "        #   [0.2, 0.1, 0.3, 0.5, 0.6, 0.9, 0.7, 0.8],  # Position 0\n",
    "        #   [0.4, 0.2, 0.1, 0.6, 0.5, 0.7, 0.9, 0.3],  # Position 1\n",
    "        #   [0.5, 0.3, 0.8, 0.2, 0.7, 0.4, 0.6, 0.1],  # Position 2\n",
    "        #   [0.9, 0.6, 0.2, 0.3, 0.1, 0.8, 0.4, 0.7]   # Position 3\n",
    "        # ]\n",
    "\n",
    "        # initial shape of x - (batch_size, seq_len)\n",
    "        # batch_size: Number of sentences in a batch\n",
    "        # seq_len: Number of tokens (words) in each sentence\n",
    "        # Each value in x is an integer index from 0 to vocab_size - 1\n",
    "        # after embedding - (batch_size, seq_len, embed_dim)\n",
    "\n",
    "        # example - embed_dim = 8, batch_size = 2\n",
    "        #     x = [\n",
    "        #   [ [0.2, 0.1, 0.4, 0.3, 0.8, 0.7, 0.6, 0.9],  # Token 2\n",
    "        #     [0.5, 0.3, 0.9, 0.1, 0.2, 0.6, 0.8, 0.7],  # Token 5\n",
    "        #     [0.4, 0.9, 0.2, 0.3, 0.1, 0.7, 0.5, 0.6],  # Token 1\n",
    "        #     [0.3, 0.8, 0.6, 0.2, 0.5, 0.9, 0.7, 0.4]   # Token 7\n",
    "        #   ],  # First sentence\n",
    "\n",
    "        #   [ [0.1, 0.6, 0.9, 0.7, 0.3, 0.5, 0.2, 0.8],  # Token 0\n",
    "        #     [0.4, 0.2, 0.3, 0.9, 0.7, 0.5, 0.1, 0.6],  # Token 3\n",
    "        #     [0.8, 0.5, 0.4, 0.1, 0.6, 0.3, 0.2, 0.7],  # Token 8\n",
    "        #     [0.9, 0.3, 0.5, 0.7, 0.8, 0.2, 0.6, 0.1]   # Token 4\n",
    "        #   ]   # Second sentence\n",
    "        # ]\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        # the maximum sequence length the model can handle\n",
    "        maxlen = tf.shape(x)[-1] # sets maxlen to the length of the input sequence\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1) # Generate [0, 1, 2, ..., maxlen-1]\n",
    "        positions = self.pos_emb(positions) # Each position index is mapped to a trainable embedding of shape (maxlen, embed_dim)\n",
    "        x = self.token_emb(x) # Each token ID in x is mapped to an embedding of shape (batch_size, maxlen, embed_dim)\n",
    "        return x + positions\n",
    "\n",
    "        # x has shape (batch_size, seq_len, embed_dim)\n",
    "        # positions has shape (maxlen, embed_dim)\n",
    "        # But maxlen == seq_len, so positions effectively has shape (seq_len, embed_dim).\n",
    "        # TensorFlow broadcasts positions across batch_size, treating it as if it were (1, seq_len, embed_dim).\n",
    "        # This allows element-wise addition between x and position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90e2b803-b3f9-4d5d-9276-765e559cc3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 50, 128)\n",
      "(None, 50, 128)\n",
      "(None, 128)\n",
      "(None, 6663)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ token_and_position_embedding    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">859,264</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">198,272</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ get_item (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6663</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">859,527</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ token_and_position_embedding    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m859,264\u001b[0m │\n",
       "│ (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m198,272\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ get_item (\u001b[38;5;33mGetItem\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6663\u001b[0m)           │       \u001b[38;5;34m859,527\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,917,063</span> (7.31 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,917,063\u001b[0m (7.31 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,917,063</span> (7.31 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,917,063\u001b[0m (7.31 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model Parameters\n",
    "embed_dim = 128  # Embedding size\n",
    "num_heads = 4    # Number of attention heads\n",
    "ff_dim = 512     # Feed-forward layer size\n",
    "maxlen = seq_length # here it is 50 defined above\n",
    "\n",
    "# below total words = 6662 (see above - basically all tokens in the text)\n",
    "\n",
    "# Build the model\n",
    "inputs = tf.keras.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, total_words, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "print(x.shape)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x, training=True)\n",
    "print(x.shape)\n",
    "x = x[:, -1, :]\n",
    "print(x.shape)\n",
    "x = Dense(total_words, activation=\"softmax\")(x)\n",
    "print(x.shape)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a1ddfe9-3cfd-4a7e-9788-2b4a21fc4f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 34ms/step - accuracy: 0.0838 - loss: 6.5015 \n",
      "Epoch 2/10\n",
      "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 33ms/step - accuracy: 0.1598 - loss: 5.0763 \n",
      "Epoch 3/10\n",
      "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 45ms/step - accuracy: 0.2092 - loss: 4.2887\n",
      "Epoch 4/10\n",
      "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m169s\u001b[0m 67ms/step - accuracy: 0.2596 - loss: 3.6359 \n",
      "Epoch 5/10\n",
      "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 33ms/step - accuracy: 0.3211 - loss: 3.1041 \n",
      "Epoch 6/10\n",
      "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 33ms/step - accuracy: 0.4000 - loss: 2.6049 \n",
      "Epoch 7/10\n",
      "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 33ms/step - accuracy: 0.4808 - loss: 2.1647 \n",
      "Epoch 8/10\n",
      "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 33ms/step - accuracy: 0.5480 - loss: 1.8180 \n",
      "Epoch 9/10\n",
      "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 32ms/step - accuracy: 0.6199 - loss: 1.4922 \n",
      "Epoch 10/10\n",
      "\u001b[1m2531/2531\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 33ms/step - accuracy: 0.6674 - loss: 1.2643 \n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, batch_size=32, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a50e593-9fea-4069-bc5d-c890fda3a1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268\n",
      "harry looked at the black haired man in there was a very stern face and cold and took a few seconds and then said filch running around looking at them there was a big old fashioned one — probably silver like that they had never even put such a wonderful sleek and tall\n"
     ]
    }
   ],
   "source": [
    "def generate_text(seed_text, next_words, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len - 1, padding='pre')\n",
    "        predicted = model.predict(token_list, verbose=0)\n",
    "        predicted_word = tokenizer.index_word[np.argmax(predicted)]\n",
    "        seed_text += \" \" + predicted_word\n",
    "    return seed_text\n",
    "\n",
    "# Generate text\n",
    "seed_text = \"harry looked at\"\n",
    "generated_text = generate_text(seed_text, next_words=50, max_sequence_len=seq_length + 1)\n",
    "print(len(generated_text))\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-env)",
   "language": "python",
   "name": "tf-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
